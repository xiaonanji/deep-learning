{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.7.0', True, '4.5.1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import re\n",
    "import random\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "torch.__version__, torch.cuda.is_available(), transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 1\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "# PRETRAIN_MODEL = 'bert-base-cased' #'../input/pytorch-bert-ner-2/pytorch_bert_ner_model'\n",
    "TRUNCATING_TYPE = 'pre'\n",
    "PADDING_TYPE = 'post'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device, n_gpu)\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRETRAIN_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained('../input/tokenizer/tokenizer')\n",
    "model = BertForTokenClassification.from_pretrained('../input/pytorch-bert-ner-model-v2/pytorch_bert_ner_model_v2')\n",
    "\n",
    "# model = BertForTokenClassification.from_pretrained('../input/pytorch-bert-ner/pytorch_bert_ner_model')\n",
    "model = model.to(device)\n",
    "\n",
    "tag_values = ['O', 'B-D', 'I-D', 'PAD']\n",
    "\n",
    "# Kaggle provided func to clean the dataset names\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def correct_word_broken(tu):\n",
    "    for i in range(len(tu)):\n",
    "        (token, tag) = tu[i]\n",
    "        if i > 0:\n",
    "            (previous_token, previous_tag) = tu[i-1]\n",
    "            if previous_tag == 'B-D' and token.startswith('##'):\n",
    "                tu[i] = (token, previous_tag)\n",
    "    return tu\n",
    "\n",
    "def remove_super_tags(tags):\n",
    "    new_set = set()\n",
    "    for s in tags:\n",
    "        remove = False\n",
    "        for ss in tags:\n",
    "            if ss in s and s > ss:\n",
    "                remove = True\n",
    "                break\n",
    "        if not remove:\n",
    "            new_set.add(s)\n",
    "\n",
    "    return new_set\n",
    "\n",
    "def get_dataset_tags(tokenized_sentence, pred_tags):\n",
    "    dataset_names = set()\n",
    "    dataset_name = ''\n",
    "    tu = correct_word_broken(list(zip(tokenized_sentence, pred_tags)))\n",
    "    found_start = False\n",
    "    for (token, tag) in tu:\n",
    "        if not found_start and tag == 'B-D' and not token.startswith('##'): # Found the starting position\n",
    "            dataset_name += token\n",
    "            found_start = True\n",
    "            continue\n",
    "        \n",
    "        if found_start:\n",
    "            if tag == 'B-D' or tag == 'I-D':\n",
    "                if token.startswith('##'):\n",
    "                    dataset_name += token.replace('##', '')\n",
    "                else:\n",
    "                    dataset_name += ' ' + token\n",
    "            else:\n",
    "                found_start = False\n",
    "                dataset_name = dataset_name.strip()\n",
    "                if len(dataset_name) > 0 and (dataset_name[0] >= 'A' and dataset_name[0] <= 'Z' or dataset_name[0] >= 'a' and dataset_name[0] <= 'z') and not dataset_name.startswith('and '):    \n",
    "                    dataset_names.add(clean_text(dataset_name))\n",
    "                    dataset_name = ''\n",
    "    return dataset_names\n",
    "\n",
    "def is_sentence_worth_predict(words):\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if len(w) > 0 and w[0] >= 'A' and w[0] <= 'Z':\n",
    "            count += 1\n",
    "            if count >= 3:\n",
    "                return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce the scoring time, use pytorch's dataloader to load in sentences and use batch model for prediction\n",
    "class ScoringDataset:\n",
    "    def __init__(self, fids, sids, input_ids, masks):\n",
    "        self.fids = fids\n",
    "        self.sids = sids\n",
    "        self.input_ids = input_ids\n",
    "        self.masks = masks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        b_fids = self.fids[item]\n",
    "        b_sids = self.sids[item]\n",
    "        b_input_ids = self.input_ids[item]\n",
    "        b_masks = self.masks[item]\n",
    "        \n",
    "#         print(b_fids)\n",
    "#         print(b_sids)\n",
    "#         print(b_input_ids)\n",
    "#         print(b_tokenized_sentences)\n",
    "#         print(b_masks)\n",
    "\n",
    "        return {\n",
    "            \"b_fids\": b_fids,\n",
    "            \"b_sids\": b_sids,\n",
    "            \"b_input_ids\": torch.tensor(b_input_ids, dtype=torch.long),\n",
    "            \"b_masks\": torch.tensor(b_masks, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This func takes dataloader as input for prediction\n",
    "def predict_dl(scoring_dataloader, sentence_df):\n",
    "    predicted_tags = []\n",
    "    print(f'In total {len(scoring_dataloader)} batches to process')\n",
    "    processed = 0\n",
    "    model.eval()\n",
    "    for batch in scoring_dataloader:\n",
    "        b_fids = batch['b_fids']\n",
    "        b_sids = batch['b_sids']\n",
    "        b_input_ids = batch['b_input_ids'].to(device).to(torch.int64)\n",
    "        b_input_masks = batch['b_masks'].to(device).to(torch.int64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_masks)\n",
    "        logits = outputs[0].detach().cpu().numpy()\n",
    "#         print(logits)\n",
    "        predictions = [list(p) for p in np.argmax(logits, axis=2)]\n",
    "#         print(predictions)\n",
    "        pred_tags = []\n",
    "        for r in predictions:\n",
    "            pred_tags.append([tag_values[ri] for ri in r])\n",
    "    \n",
    "        for fid, sid, pt in zip(b_fids, b_sids, pred_tags):\n",
    "            sentence = sentence_df[sentence_df['sid']==sid]['sentence'].values[0]\n",
    "            if 'B-D' in pt or 'I-D' in pt:\n",
    "                tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "                dataset_names = get_dataset_tags(tokenized_sentence, pt)\n",
    "                predicted_tags.append((fid, sid, '|'.join(dataset_names)))\n",
    "            else:\n",
    "                if \"Alzheimer's Disease Neuroimaging Initiative (ADNI)\" in sentence:\n",
    "                    predicted_tags.append((fid, sid, clean_text(\"Alzheimer's Disease Neuroimaging Initiative (ADNI)\")))\n",
    "                elif 'ADNI' in sentence:\n",
    "                    predicted_tags.append((fid, sid, 'adni'))\n",
    "                else:\n",
    "                    predicted_tags.append((fid, sid, ''))\n",
    "                \n",
    "        processed += 1\n",
    "        if processed % 100 == 0:\n",
    "            print(f'{processed} processed')\n",
    "     \n",
    "    print('All batches processed')\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(sentences):\n",
    "    sentence_df = pd.DataFrame.from_records(sentences, columns =['fid', 'sid', 'sentence'])\n",
    "    \n",
    "    # Prepare bert tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    fids = []\n",
    "    sids = []\n",
    "\n",
    "    for index, row in sentence_df.iterrows():\n",
    "        fid = row['fid']\n",
    "        sid = row['sid']\n",
    "        sentence = row['sentence']\n",
    "        tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "        fids.append(fid)\n",
    "        sids.append(sid)\n",
    "\n",
    "    input_ids = pad_sequences(\n",
    "        [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_sentences], \n",
    "        maxlen=MAX_LEN, \n",
    "        dtype='long', \n",
    "        value=0.0, \n",
    "        truncating=TRUNCATING_TYPE, \n",
    "        padding=PADDING_TYPE\n",
    "    )\n",
    "\n",
    "    attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "    scoring_dataset = ScoringDataset(\n",
    "        fids=fids,\n",
    "        sids=sids,\n",
    "        input_ids=input_ids,\n",
    "        masks=attention_masks,\n",
    "    )\n",
    "\n",
    "    scoring_dataloader = DataLoader(\n",
    "        scoring_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    pred = predict_dl(scoring_dataloader, sentence_df)\n",
    "    pred_df = pd.DataFrame.from_records(pred, columns =['fid', 'sid', 'predicted'])\n",
    "    combined = pd.merge(sentence_df, pred_df, on=[\"fid\", \"sid\"])\n",
    "    \n",
    "    for index, row in combined.iterrows():\n",
    "        fid = row['fid']\n",
    "        predicted = row['predicted']\n",
    "        if fid not in result_dict:\n",
    "            result_dict[fid] = set()\n",
    "\n",
    "        if predicted is not None and predicted > '':\n",
    "            result_dict[fid].add(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Processing 8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60.json====\n",
      "====Processing 2100032a-7c33-4bff-97ef-690822c43466.json====\n",
      "====Processing 2f392438-e215-4169-bebf-21ac4ff253e1.json====\n",
      "====Processing 3f316b38-1a24-45a9-8d8c-4e05a42257c6.json====\n",
      "In total 64 batches to process\n",
      "All batches processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "sentences = []\n",
    "sid = 0\n",
    "result_dict = {}\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/coleridgeinitiative-show-us-the-data/test'):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        fid = filename.replace('.json', '')\n",
    "        print(f'====Processing {filename}====')\n",
    "        with open(file_path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "            for section in data:\n",
    "                text = section['text']\n",
    "                section_sentences = nltk.sent_tokenize(text)\n",
    "                for sentence in section_sentences:\n",
    "                    sentences.append((fid, f'S{sid}', sentence))\n",
    "                    sid += 1\n",
    "                    if len(sentences) >= BATCH_SIZE*1000:  # You can store all testing sentences in one go so break up\n",
    "                        run(sentences)\n",
    "                        sentences = []\n",
    "                        \n",
    "    if len(sentences) > 0:\n",
    "        run(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done\n"
     ]
    }
   ],
   "source": [
    "with open('./submission.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    writer.writerow(['Id','PredictionString'])\n",
    "    for key, value in result_dict.items():\n",
    "        writer.writerow([key, '|'.join(value)])\n",
    "print('All done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
