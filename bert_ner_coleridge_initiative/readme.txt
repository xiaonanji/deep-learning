This folder contains the python code for kaggle's coleridge initiative detection competition. This competition is essentially an NER problem. Given a research document, the model needs to identify all datasets referred.

The way I used to tackle this problem is to break each document into sentences and identify datasets mentioned in each sentence. Of course by isolating the NER problem into sentence-wise scope some context information is lost but I hope it wouldn't affect the overall performance much. Anyway if using BERT, there is a sequence length restriction as well and this restriction is very easily broken if we consider longer word sequences.

The competition provides 14316 training documents but only 5 testing documents as a sample to allow you verify your output format. So we need to split the training data into testing ourselves. The first challenge of this competition is to prepare the NER training data. The training data only provides the datasets on document level so we need to use this information to identify datasets mentioned in sentence level. I also notice that the dataset labels provided contain errors and noise. If not properly processed, this noise will affect the model performance. So the first step is to go through the training data and create sentence-level training data in the proper format. This is performed in the data_prep jupter notebook.

Not every sentence mentions a dataset. So shall I include some sentences that have no dataset mentioned in the training data or shall I only include sentences that mention datasets? Given that in the scoring I will encounter many sentences that don't mention any datasets, I should include sentences that mention no dataset. Experiment shows that this choice yields much better model performance. However, this so called 'negative' samples are too big, in fact, only a small portion of the sentences mention some datasets and majority sentences are 'negative' samples. To control the training time, I have to setup criteria to select worth-trying negative samples. This is also shown in the code.

The model I chose is Huggingface's Bert. These days Bert and its variants show great performance in NER problem. To use Bert, we need to tokenize each sentence, pad the sequences to the same length (I chose 100 words) and convert each token into index. I used BOI encoding, which is typical in NER problem. The only choice I need to make is how to deal with Bert's word-wise tokenizing. The tokenization method in Bert is different from traditional. Bert will break a word into subwords during tokenization. For example, Bert may tokenize the word 'respondents' into 'respond' and '##ents'. The double hash indicates that the second word is not a separate word but a subword. Given this tokenization method, how should I apply the BOI encoding? Suppose a sentence 'The experiment uses Weka Medical Database as primary data source', If each word is tokenized by itself, the BOI encoding should be 'The(O) experiment(O) uses(O) Weka(B) Medical(I) Database(I) as(O) primary(O) data(O) source(O)'. However, if Bert breaks 'Weka' into 'We' and '##ka', how should I assign BOI to each subwords? There are general two ways. The first is to introduce a new tag 'X' and give to the subword. The second is to assign the first subword's tag to the subsequential subwords. In this competition, I chose the latter way, just to keep the number of tags as simple as possible.

The last challenge of this competition is Kaggle's submission restriction. Kaggle provides a hidden testing dataset containing 8000 documents. The user has to submit the notebook and Kaggle will automatically run the code on the testing dataset. The total runtime cannot exceed 9 hours and there are other restrictions such as memory size. For many of my submissions I either exceed the limit of computation or exceed the runtime. There is no result produced on these failed submissions and the error messages sometimes are not very clear. To my opinion this Kaggle-hosted coding environment and submission system introduces extra difficulties to the users and one definitely needs sometime to get use to it. Given that Bert is slow to run, I have to use batch scoring to reduce the scoring time.

The notebooks are:
1. data_prep: transform the original input into my training data
2. training: training the bert model
3. submission: score the testing data using the trained model

data_prep and submission notebooks were run on Kaggle's online notebook environment. Once the desired training data is produced, I downloaded it to my local machine to train the model.
