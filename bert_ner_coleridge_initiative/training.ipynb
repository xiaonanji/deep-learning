{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NER data formatting process is in kaggle's kernel:\n",
    "### https://www.kaggle.com/xiaonanji/coleridge-initiative/edit\n",
    "\n",
    "The kaggle kernel has limited memory to train bert so I have to do it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.7.1+cu110', True, '4.2.2')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import re\n",
    "import random\n",
    "\n",
    "torch.__version__, torch.cuda.is_available(), transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8031]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8031]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "def get_gpu_memory():\n",
    "    _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "    ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "    print(memory_free_values)\n",
    "    return memory_free_values\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21363823, 3),\n",
       "           Sentence      Word Tag\n",
       " 0         S2704290       The   O\n",
       " 1         S2704290   Centers   O\n",
       " 2         S2704290       for   O\n",
       " 3         S2704290   Disease   O\n",
       " 4         S2704290   Control   O\n",
       " ...            ...       ...  ..\n",
       " 21363818  S1137486  datasets   O\n",
       " 21363819  S1137486         [   O\n",
       " 21363820  S1137486        15   O\n",
       " 21363821  S1137486         ]   O\n",
       " 21363822  S1137486         .   O\n",
       " \n",
       " [21363823 rows x 3 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"C:\\\\Users\\\\stick\\\\kaggle\\\\ner_training.csv\",sep=\",\",encoding=\"utf8\", header=None, keep_default_na=False, na_values=[], names=['Sentence', 'Word', 'Tag'])\n",
    "training_data.shape, training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s['Word'].values.tolist(),\n",
    "                                                     s['Tag'].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped['s{0}'.format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'study', 'used', 'data', 'from', 'the', 'National', 'Education', 'Longitudinal', 'Study', '(', 'NELS:88', ')', 'to', 'examine', 'the', 'effects', 'of', 'dual', 'enrollment', 'programs', 'for', 'high', 'school', 'students', 'on', 'college', 'degree', 'attainment', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B-D', 'I-D', 'I-D', 'I-D', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences = [[tu[0] for tu in sentence] for sentence in getter.sentences]\n",
    "print(sentences[0])\n",
    "labels = [[tu[1] for tu in sentence] for sentence in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434805, 434805)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-D': 1, 'I-D': 2, 'PAD': 3}\n",
      "['O', 'B-D', 'I-D', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "tag_values = list(training_data.Tag.unique())\n",
    "tag_values.append('PAD')\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "print(tag2idx)\n",
    "print(tag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "PRETRAIN_MODEL = 'bert-base-cased'\n",
    "VALIDATE_PERCENT = 0.2\n",
    "TRUNCATING_TYPE = 'pre'\n",
    "PADDING_TYPE = 'post'\n",
    "epochs = 2\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(device)\n",
    "print(n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAIN_MODEL, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    \n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        try:\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            n_subwords = len(tokenized_word)\n",
    "            tokenized_sentence.extend(tokenized_word)\n",
    "            labels.extend([label]*n_subwords)\n",
    "        except AttributeError:\n",
    "            print(word)\n",
    "            print(sentence)\n",
    "            print(text_labels)\n",
    "            \n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434805"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(\n",
    "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "    maxlen=MAX_LEN, \n",
    "    dtype='long', \n",
    "    value=0.0, \n",
    "    truncating=TRUNCATING_TYPE, \n",
    "    padding=PADDING_TYPE\n",
    ")\n",
    "tags = pad_sequences(\n",
    "    [[tag2idx.get(l) for l in lab] for lab in labels], \n",
    "    maxlen=MAX_LEN, \n",
    "    dtype='long',\n",
    "    value=tag2idx[\"PAD\"],\n",
    "    truncating=TRUNCATING_TYPE,\n",
    "    padding=PADDING_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7866, 2846, 1409, 1191, 5844, 27451, 26547, 2559, 26909]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['difficulties', 'difficult', 'If', 'if', 'AD', '##NI', 'RA', '##V', '##LT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434805, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags, tr_masks, val_masks = train_test_split(\n",
    "    input_ids, tags, attention_masks, \n",
    "    random_state=2021,\n",
    "    test_size=VALIDATE_PERCENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((347844, 100), (347844, 100), 347844)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs.shape, tr_tags.shape, len(tr_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((86961, 100), (86961, 100), 86961)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inputs.shape, val_tags.shape, len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10605,  6824,  4001,  1132,  7160,  1114,  5021,  2942,   117,\n",
       "        1423,  3107, 19755,  1132,  2602,  1107,  1843,  2448,   117,\n",
       "       19755,  2766,  1104,   123,   118,  3527, 11080,  1132,  2533,\n",
       "        1118,  4348,  2448,  1105, 19755,  2766,  1104,   124,   116,\n",
       "        3527, 11080,  1132,  3597,  1114, 13552,  2942,   113,  1267,\n",
       "        6177,   123,   119,   125,   157, 19366,  1204, 15577, 17580,\n",
       "        4800,  1111, 14441,  1372, 14256,   114,   119,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_inputs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_inputs = torch.tensor(tr_inputs)\n",
    "# val_inputs = torch.tensor(val_inputs)\n",
    "# tr_tags = torch.tensor(tr_tags)\n",
    "# val_tags = torch.tensor(val_tags)\n",
    "# tr_masks = torch.tensor(tr_masks)\n",
    "# val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeanDataset:\n",
    "    def __init__(self, word_tokens, masks, tags):\n",
    "        self.word_tokens = word_tokens\n",
    "        self.masks = masks\n",
    "        self.tags = tags\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_tokens)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        word_tokens = self.word_tokens[item]\n",
    "        masks = self.masks[item]\n",
    "        tags = self.tags[item]\n",
    "        return {\n",
    "            \"word_tokens\": torch.tensor(word_tokens, dtype=torch.long),\n",
    "            \"masks\": torch.tensor(masks, dtype=torch.float),\n",
    "            \"tags\": torch.tensor(tags, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "\n",
    "train_dataset = SeanDataset(\n",
    "    word_tokens=tr_inputs,\n",
    "    masks=tr_masks,\n",
    "    tags=tr_tags\n",
    ")\n",
    "    \n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "# valid_sampler = SequentialSampler(valid_data)\n",
    "# valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = SeanDataset(\n",
    "    word_tokens=val_inputs,\n",
    "    masks=val_masks,\n",
    "    tags=val_tags\n",
    ")\n",
    "    \n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10871"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_tokens': tensor([[ 1335,   139, 15928,  ...,     0,     0,     0],\n",
      "        [ 1109,  2812,   118,  ...,     0,     0,     0],\n",
      "        [ 1370,  1859,   117,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1188,  2860,  1110,  ...,     0,     0,     0],\n",
      "        [  138,  8362, 12416,  ...,     0,     0,     0],\n",
      "        [ 1130,  2943,   123,  ...,     0,     0,     0]]), 'masks': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]]), 'tags': tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3]])}\n"
     ]
    }
   ],
   "source": [
    "for d in train_dataloader:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    PRETRAIN_MODEL,\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7108]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7108]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21742\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "print(total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "Batch    40 of 10,871. Elapsed: 0:00:16.\n",
      "Batch    80 of 10,871. Elapsed: 0:00:30.\n",
      "Batch   120 of 10,871. Elapsed: 0:00:45.\n",
      "Batch   160 of 10,871. Elapsed: 0:01:00.\n",
      "Batch   200 of 10,871. Elapsed: 0:01:15.\n",
      "Batch   240 of 10,871. Elapsed: 0:01:30.\n",
      "Batch   280 of 10,871. Elapsed: 0:01:46.\n",
      "Batch   320 of 10,871. Elapsed: 0:02:01.\n",
      "Batch   360 of 10,871. Elapsed: 0:02:16.\n",
      "Batch   400 of 10,871. Elapsed: 0:02:31.\n",
      "Batch   440 of 10,871. Elapsed: 0:02:46.\n",
      "Batch   480 of 10,871. Elapsed: 0:03:01.\n",
      "Batch   520 of 10,871. Elapsed: 0:03:17.\n",
      "Batch   560 of 10,871. Elapsed: 0:03:32.\n",
      "Batch   600 of 10,871. Elapsed: 0:03:47.\n",
      "Batch   640 of 10,871. Elapsed: 0:04:02.\n",
      "Batch   680 of 10,871. Elapsed: 0:04:17.\n",
      "Batch   720 of 10,871. Elapsed: 0:04:33.\n",
      "Batch   760 of 10,871. Elapsed: 0:04:48.\n",
      "Batch   800 of 10,871. Elapsed: 0:05:03.\n",
      "Batch   840 of 10,871. Elapsed: 0:05:18.\n",
      "Batch   880 of 10,871. Elapsed: 0:05:34.\n",
      "Batch   920 of 10,871. Elapsed: 0:05:49.\n",
      "Batch   960 of 10,871. Elapsed: 0:06:04.\n",
      "Batch 1,000 of 10,871. Elapsed: 0:06:20.\n",
      "Batch 1,040 of 10,871. Elapsed: 0:06:35.\n",
      "Batch 1,080 of 10,871. Elapsed: 0:06:51.\n",
      "Batch 1,120 of 10,871. Elapsed: 0:07:06.\n",
      "Batch 1,160 of 10,871. Elapsed: 0:07:21.\n",
      "Batch 1,200 of 10,871. Elapsed: 0:07:37.\n",
      "Batch 1,240 of 10,871. Elapsed: 0:07:52.\n",
      "Batch 1,280 of 10,871. Elapsed: 0:08:08.\n",
      "Batch 1,320 of 10,871. Elapsed: 0:08:23.\n",
      "Batch 1,360 of 10,871. Elapsed: 0:08:39.\n",
      "Batch 1,400 of 10,871. Elapsed: 0:08:54.\n",
      "Batch 1,440 of 10,871. Elapsed: 0:09:09.\n",
      "Batch 1,480 of 10,871. Elapsed: 0:09:25.\n",
      "Batch 1,520 of 10,871. Elapsed: 0:09:40.\n",
      "Batch 1,560 of 10,871. Elapsed: 0:09:56.\n",
      "Batch 1,600 of 10,871. Elapsed: 0:10:11.\n",
      "Batch 1,640 of 10,871. Elapsed: 0:10:26.\n",
      "Batch 1,680 of 10,871. Elapsed: 0:10:42.\n",
      "Batch 1,720 of 10,871. Elapsed: 0:10:57.\n",
      "Batch 1,760 of 10,871. Elapsed: 0:11:13.\n",
      "Batch 1,800 of 10,871. Elapsed: 0:11:28.\n",
      "Batch 1,840 of 10,871. Elapsed: 0:11:44.\n",
      "Batch 1,880 of 10,871. Elapsed: 0:11:59.\n",
      "Batch 1,920 of 10,871. Elapsed: 0:12:15.\n",
      "Batch 1,960 of 10,871. Elapsed: 0:12:30.\n",
      "Batch 2,000 of 10,871. Elapsed: 0:12:46.\n",
      "Batch 2,040 of 10,871. Elapsed: 0:13:01.\n",
      "Batch 2,080 of 10,871. Elapsed: 0:13:17.\n",
      "Batch 2,120 of 10,871. Elapsed: 0:13:32.\n",
      "Batch 2,160 of 10,871. Elapsed: 0:13:47.\n",
      "Batch 2,200 of 10,871. Elapsed: 0:14:03.\n",
      "Batch 2,240 of 10,871. Elapsed: 0:14:18.\n",
      "Batch 2,280 of 10,871. Elapsed: 0:14:34.\n",
      "Batch 2,320 of 10,871. Elapsed: 0:14:49.\n",
      "Batch 2,360 of 10,871. Elapsed: 0:15:05.\n",
      "Batch 2,400 of 10,871. Elapsed: 0:15:20.\n",
      "Batch 2,440 of 10,871. Elapsed: 0:15:36.\n",
      "Batch 2,480 of 10,871. Elapsed: 0:15:51.\n",
      "Batch 2,520 of 10,871. Elapsed: 0:16:07.\n",
      "Batch 2,560 of 10,871. Elapsed: 0:16:22.\n",
      "Batch 2,600 of 10,871. Elapsed: 0:16:38.\n",
      "Batch 2,640 of 10,871. Elapsed: 0:16:53.\n",
      "Batch 2,680 of 10,871. Elapsed: 0:17:09.\n",
      "Batch 2,720 of 10,871. Elapsed: 0:17:24.\n",
      "Batch 2,760 of 10,871. Elapsed: 0:17:40.\n",
      "Batch 2,800 of 10,871. Elapsed: 0:17:55.\n",
      "Batch 2,840 of 10,871. Elapsed: 0:18:11.\n",
      "Batch 2,880 of 10,871. Elapsed: 0:18:26.\n",
      "Batch 2,920 of 10,871. Elapsed: 0:18:42.\n",
      "Batch 2,960 of 10,871. Elapsed: 0:18:58.\n",
      "Batch 3,000 of 10,871. Elapsed: 0:19:13.\n",
      "Batch 3,040 of 10,871. Elapsed: 0:19:29.\n",
      "Batch 3,080 of 10,871. Elapsed: 0:19:44.\n",
      "Batch 3,120 of 10,871. Elapsed: 0:20:00.\n",
      "Batch 3,160 of 10,871. Elapsed: 0:20:15.\n",
      "Batch 3,200 of 10,871. Elapsed: 0:20:31.\n",
      "Batch 3,240 of 10,871. Elapsed: 0:20:46.\n",
      "Batch 3,280 of 10,871. Elapsed: 0:21:02.\n",
      "Batch 3,320 of 10,871. Elapsed: 0:21:17.\n",
      "Batch 3,360 of 10,871. Elapsed: 0:21:33.\n",
      "Batch 3,400 of 10,871. Elapsed: 0:21:48.\n",
      "Batch 3,440 of 10,871. Elapsed: 0:22:04.\n",
      "Batch 3,480 of 10,871. Elapsed: 0:22:19.\n",
      "Batch 3,520 of 10,871. Elapsed: 0:22:35.\n",
      "Batch 3,560 of 10,871. Elapsed: 0:22:50.\n",
      "Batch 3,600 of 10,871. Elapsed: 0:23:06.\n",
      "Batch 3,640 of 10,871. Elapsed: 0:23:22.\n",
      "Batch 3,680 of 10,871. Elapsed: 0:23:37.\n",
      "Batch 3,720 of 10,871. Elapsed: 0:23:53.\n",
      "Batch 3,760 of 10,871. Elapsed: 0:24:08.\n",
      "Batch 3,800 of 10,871. Elapsed: 0:24:24.\n",
      "Batch 3,840 of 10,871. Elapsed: 0:24:39.\n",
      "Batch 3,880 of 10,871. Elapsed: 0:24:55.\n",
      "Batch 3,920 of 10,871. Elapsed: 0:25:10.\n",
      "Batch 3,960 of 10,871. Elapsed: 0:25:26.\n",
      "Batch 4,000 of 10,871. Elapsed: 0:25:41.\n",
      "Batch 4,040 of 10,871. Elapsed: 0:25:57.\n",
      "Batch 4,080 of 10,871. Elapsed: 0:26:12.\n",
      "Batch 4,120 of 10,871. Elapsed: 0:26:28.\n",
      "Batch 4,160 of 10,871. Elapsed: 0:26:44.\n",
      "Batch 4,200 of 10,871. Elapsed: 0:26:59.\n",
      "Batch 4,240 of 10,871. Elapsed: 0:27:15.\n",
      "Batch 4,280 of 10,871. Elapsed: 0:27:30.\n",
      "Batch 4,320 of 10,871. Elapsed: 0:27:46.\n",
      "Batch 4,360 of 10,871. Elapsed: 0:28:01.\n",
      "Batch 4,400 of 10,871. Elapsed: 0:28:17.\n",
      "Batch 4,440 of 10,871. Elapsed: 0:28:32.\n",
      "Batch 4,480 of 10,871. Elapsed: 0:28:48.\n",
      "Batch 4,520 of 10,871. Elapsed: 0:29:03.\n",
      "Batch 4,560 of 10,871. Elapsed: 0:29:19.\n",
      "Batch 4,600 of 10,871. Elapsed: 0:29:35.\n",
      "Batch 4,640 of 10,871. Elapsed: 0:29:50.\n",
      "Batch 4,680 of 10,871. Elapsed: 0:30:06.\n",
      "Batch 4,720 of 10,871. Elapsed: 0:30:21.\n",
      "Batch 4,760 of 10,871. Elapsed: 0:30:37.\n",
      "Batch 4,800 of 10,871. Elapsed: 0:30:52.\n",
      "Batch 4,840 of 10,871. Elapsed: 0:31:08.\n",
      "Batch 4,880 of 10,871. Elapsed: 0:31:23.\n",
      "Batch 4,920 of 10,871. Elapsed: 0:31:39.\n",
      "Batch 4,960 of 10,871. Elapsed: 0:31:55.\n",
      "Batch 5,000 of 10,871. Elapsed: 0:32:10.\n",
      "Batch 5,040 of 10,871. Elapsed: 0:32:26.\n",
      "Batch 5,080 of 10,871. Elapsed: 0:32:41.\n",
      "Batch 5,120 of 10,871. Elapsed: 0:32:57.\n",
      "Batch 5,160 of 10,871. Elapsed: 0:33:12.\n",
      "Batch 5,200 of 10,871. Elapsed: 0:33:28.\n",
      "Batch 5,240 of 10,871. Elapsed: 0:33:43.\n",
      "Batch 5,280 of 10,871. Elapsed: 0:33:59.\n",
      "Batch 5,320 of 10,871. Elapsed: 0:34:15.\n",
      "Batch 5,360 of 10,871. Elapsed: 0:34:30.\n",
      "Batch 5,400 of 10,871. Elapsed: 0:34:46.\n",
      "Batch 5,440 of 10,871. Elapsed: 0:35:01.\n",
      "Batch 5,480 of 10,871. Elapsed: 0:35:17.\n",
      "Batch 5,520 of 10,871. Elapsed: 0:35:32.\n",
      "Batch 5,560 of 10,871. Elapsed: 0:35:48.\n",
      "Batch 5,600 of 10,871. Elapsed: 0:36:03.\n",
      "Batch 5,640 of 10,871. Elapsed: 0:36:19.\n",
      "Batch 5,680 of 10,871. Elapsed: 0:36:35.\n",
      "Batch 5,720 of 10,871. Elapsed: 0:36:50.\n",
      "Batch 5,760 of 10,871. Elapsed: 0:37:06.\n",
      "Batch 5,800 of 10,871. Elapsed: 0:37:21.\n",
      "Batch 5,840 of 10,871. Elapsed: 0:37:37.\n",
      "Batch 5,880 of 10,871. Elapsed: 0:37:52.\n",
      "Batch 5,920 of 10,871. Elapsed: 0:38:08.\n",
      "Batch 5,960 of 10,871. Elapsed: 0:38:24.\n",
      "Batch 6,000 of 10,871. Elapsed: 0:38:39.\n",
      "Batch 6,040 of 10,871. Elapsed: 0:38:55.\n",
      "Batch 6,080 of 10,871. Elapsed: 0:39:10.\n",
      "Batch 6,120 of 10,871. Elapsed: 0:39:26.\n",
      "Batch 6,160 of 10,871. Elapsed: 0:39:41.\n",
      "Batch 6,200 of 10,871. Elapsed: 0:39:57.\n",
      "Batch 6,240 of 10,871. Elapsed: 0:40:12.\n",
      "Batch 6,280 of 10,871. Elapsed: 0:40:28.\n",
      "Batch 6,320 of 10,871. Elapsed: 0:40:44.\n",
      "Batch 6,360 of 10,871. Elapsed: 0:40:59.\n",
      "Batch 6,400 of 10,871. Elapsed: 0:41:15.\n",
      "Batch 6,440 of 10,871. Elapsed: 0:41:30.\n",
      "Batch 6,480 of 10,871. Elapsed: 0:41:46.\n",
      "Batch 6,520 of 10,871. Elapsed: 0:42:01.\n",
      "Batch 6,560 of 10,871. Elapsed: 0:42:17.\n",
      "Batch 6,600 of 10,871. Elapsed: 0:42:32.\n",
      "Batch 6,640 of 10,871. Elapsed: 0:42:48.\n",
      "Batch 6,680 of 10,871. Elapsed: 0:43:04.\n",
      "Batch 6,720 of 10,871. Elapsed: 0:43:19.\n",
      "Batch 6,760 of 10,871. Elapsed: 0:43:35.\n",
      "Batch 6,800 of 10,871. Elapsed: 0:43:50.\n",
      "Batch 6,840 of 10,871. Elapsed: 0:44:06.\n",
      "Batch 6,880 of 10,871. Elapsed: 0:44:21.\n",
      "Batch 6,920 of 10,871. Elapsed: 0:44:37.\n",
      "Batch 6,960 of 10,871. Elapsed: 0:44:52.\n",
      "Batch 7,000 of 10,871. Elapsed: 0:45:08.\n",
      "Batch 7,040 of 10,871. Elapsed: 0:45:24.\n",
      "Batch 7,080 of 10,871. Elapsed: 0:45:39.\n",
      "Batch 7,120 of 10,871. Elapsed: 0:45:55.\n",
      "Batch 7,160 of 10,871. Elapsed: 0:46:10.\n",
      "Batch 7,200 of 10,871. Elapsed: 0:46:26.\n",
      "Batch 7,240 of 10,871. Elapsed: 0:46:41.\n",
      "Batch 7,280 of 10,871. Elapsed: 0:46:57.\n",
      "Batch 7,320 of 10,871. Elapsed: 0:47:13.\n",
      "Batch 7,360 of 10,871. Elapsed: 0:47:28.\n",
      "Batch 7,400 of 10,871. Elapsed: 0:47:44.\n",
      "Batch 7,440 of 10,871. Elapsed: 0:47:59.\n",
      "Batch 7,480 of 10,871. Elapsed: 0:48:15.\n",
      "Batch 7,520 of 10,871. Elapsed: 0:48:30.\n",
      "Batch 7,560 of 10,871. Elapsed: 0:48:46.\n",
      "Batch 7,600 of 10,871. Elapsed: 0:49:01.\n",
      "Batch 7,640 of 10,871. Elapsed: 0:49:17.\n",
      "Batch 7,680 of 10,871. Elapsed: 0:49:33.\n",
      "Batch 7,720 of 10,871. Elapsed: 0:49:48.\n",
      "Batch 7,760 of 10,871. Elapsed: 0:50:04.\n",
      "Batch 7,800 of 10,871. Elapsed: 0:50:19.\n",
      "Batch 7,840 of 10,871. Elapsed: 0:50:35.\n",
      "Batch 7,880 of 10,871. Elapsed: 0:50:50.\n",
      "Batch 7,920 of 10,871. Elapsed: 0:51:06.\n",
      "Batch 7,960 of 10,871. Elapsed: 0:51:21.\n",
      "Batch 8,000 of 10,871. Elapsed: 0:51:37.\n",
      "Batch 8,040 of 10,871. Elapsed: 0:51:53.\n",
      "Batch 8,080 of 10,871. Elapsed: 0:52:08.\n",
      "Batch 8,120 of 10,871. Elapsed: 0:52:24.\n",
      "Batch 8,160 of 10,871. Elapsed: 0:52:39.\n",
      "Batch 8,200 of 10,871. Elapsed: 0:52:55.\n",
      "Batch 8,240 of 10,871. Elapsed: 0:53:11.\n",
      "Batch 8,280 of 10,871. Elapsed: 0:53:26.\n",
      "Batch 8,320 of 10,871. Elapsed: 0:53:42.\n",
      "Batch 8,360 of 10,871. Elapsed: 0:53:58.\n",
      "Batch 8,400 of 10,871. Elapsed: 0:54:13.\n",
      "Batch 8,440 of 10,871. Elapsed: 0:54:29.\n",
      "Batch 8,480 of 10,871. Elapsed: 0:54:44.\n",
      "Batch 8,520 of 10,871. Elapsed: 0:55:00.\n",
      "Batch 8,560 of 10,871. Elapsed: 0:55:16.\n",
      "Batch 8,600 of 10,871. Elapsed: 0:55:31.\n",
      "Batch 8,640 of 10,871. Elapsed: 0:55:47.\n",
      "Batch 8,680 of 10,871. Elapsed: 0:56:02.\n",
      "Batch 8,720 of 10,871. Elapsed: 0:56:18.\n",
      "Batch 8,760 of 10,871. Elapsed: 0:56:33.\n",
      "Batch 8,800 of 10,871. Elapsed: 0:56:49.\n",
      "Batch 8,840 of 10,871. Elapsed: 0:57:05.\n",
      "Batch 8,880 of 10,871. Elapsed: 0:57:20.\n",
      "Batch 8,920 of 10,871. Elapsed: 0:57:36.\n",
      "Batch 8,960 of 10,871. Elapsed: 0:57:51.\n",
      "Batch 9,000 of 10,871. Elapsed: 0:58:07.\n",
      "Batch 9,040 of 10,871. Elapsed: 0:58:22.\n",
      "Batch 9,080 of 10,871. Elapsed: 0:58:38.\n",
      "Batch 9,120 of 10,871. Elapsed: 0:58:54.\n",
      "Batch 9,160 of 10,871. Elapsed: 0:59:09.\n",
      "Batch 9,200 of 10,871. Elapsed: 0:59:25.\n",
      "Batch 9,240 of 10,871. Elapsed: 0:59:40.\n",
      "Batch 9,280 of 10,871. Elapsed: 0:59:56.\n",
      "Batch 9,320 of 10,871. Elapsed: 1:00:11.\n",
      "Batch 9,360 of 10,871. Elapsed: 1:00:27.\n",
      "Batch 9,400 of 10,871. Elapsed: 1:00:43.\n",
      "Batch 9,440 of 10,871. Elapsed: 1:00:58.\n",
      "Batch 9,480 of 10,871. Elapsed: 1:01:14.\n",
      "Batch 9,520 of 10,871. Elapsed: 1:01:29.\n",
      "Batch 9,560 of 10,871. Elapsed: 1:01:45.\n",
      "Batch 9,600 of 10,871. Elapsed: 1:02:00.\n",
      "Batch 9,640 of 10,871. Elapsed: 1:02:16.\n",
      "Batch 9,680 of 10,871. Elapsed: 1:02:31.\n",
      "Batch 9,720 of 10,871. Elapsed: 1:02:47.\n",
      "Batch 9,760 of 10,871. Elapsed: 1:03:02.\n",
      "Batch 9,800 of 10,871. Elapsed: 1:03:18.\n",
      "Batch 9,840 of 10,871. Elapsed: 1:03:34.\n",
      "Batch 9,880 of 10,871. Elapsed: 1:03:49.\n",
      "Batch 9,920 of 10,871. Elapsed: 1:04:05.\n",
      "Batch 9,960 of 10,871. Elapsed: 1:04:20.\n",
      "Batch 10,000 of 10,871. Elapsed: 1:04:36.\n",
      "Batch 10,040 of 10,871. Elapsed: 1:04:51.\n",
      "Batch 10,080 of 10,871. Elapsed: 1:05:07.\n",
      "Batch 10,120 of 10,871. Elapsed: 1:05:23.\n",
      "Batch 10,160 of 10,871. Elapsed: 1:05:38.\n",
      "Batch 10,200 of 10,871. Elapsed: 1:05:54.\n",
      "Batch 10,240 of 10,871. Elapsed: 1:06:09.\n",
      "Batch 10,280 of 10,871. Elapsed: 1:06:25.\n",
      "Batch 10,320 of 10,871. Elapsed: 1:06:40.\n",
      "Batch 10,360 of 10,871. Elapsed: 1:06:56.\n",
      "Batch 10,400 of 10,871. Elapsed: 1:07:11.\n",
      "Batch 10,440 of 10,871. Elapsed: 1:07:27.\n",
      "Batch 10,480 of 10,871. Elapsed: 1:07:43.\n",
      "Batch 10,520 of 10,871. Elapsed: 1:07:58.\n",
      "Batch 10,560 of 10,871. Elapsed: 1:08:14.\n",
      "Batch 10,600 of 10,871. Elapsed: 1:08:29.\n",
      "Batch 10,640 of 10,871. Elapsed: 1:08:45.\n",
      "Batch 10,680 of 10,871. Elapsed: 1:09:01.\n",
      "Batch 10,720 of 10,871. Elapsed: 1:09:16.\n",
      "Batch 10,760 of 10,871. Elapsed: 1:09:32.\n",
      "Batch 10,800 of 10,871. Elapsed: 1:09:47.\n",
      "Batch 10,840 of 10,871. Elapsed: 1:10:03.\n",
      "\n",
      "Average training loss: 0.00\n",
      "Training epoch took: 1:10:15\n",
      "\n",
      "Running Validation...\n",
      "Validation loss: 0.002309874899854427\n",
      "Validation Accuracy: 0.9993469731754844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [1:16:19<1:16:19, 4579.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.9825051455454279\n",
      "\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "Batch    40 of 10,871. Elapsed: 0:00:15.\n",
      "Batch    80 of 10,871. Elapsed: 0:00:31.\n",
      "Batch   120 of 10,871. Elapsed: 0:00:46.\n",
      "Batch   160 of 10,871. Elapsed: 0:01:02.\n",
      "Batch   200 of 10,871. Elapsed: 0:01:17.\n",
      "Batch   240 of 10,871. Elapsed: 0:01:33.\n",
      "Batch   280 of 10,871. Elapsed: 0:01:48.\n",
      "Batch   320 of 10,871. Elapsed: 0:02:04.\n",
      "Batch   360 of 10,871. Elapsed: 0:02:19.\n",
      "Batch   400 of 10,871. Elapsed: 0:02:35.\n",
      "Batch   440 of 10,871. Elapsed: 0:02:50.\n",
      "Batch   480 of 10,871. Elapsed: 0:03:06.\n",
      "Batch   520 of 10,871. Elapsed: 0:03:21.\n",
      "Batch   560 of 10,871. Elapsed: 0:03:37.\n",
      "Batch   600 of 10,871. Elapsed: 0:03:52.\n",
      "Batch   640 of 10,871. Elapsed: 0:04:08.\n",
      "Batch   680 of 10,871. Elapsed: 0:04:23.\n",
      "Batch   720 of 10,871. Elapsed: 0:04:39.\n",
      "Batch   760 of 10,871. Elapsed: 0:04:55.\n",
      "Batch   800 of 10,871. Elapsed: 0:05:10.\n",
      "Batch   840 of 10,871. Elapsed: 0:05:26.\n",
      "Batch   880 of 10,871. Elapsed: 0:05:41.\n",
      "Batch   920 of 10,871. Elapsed: 0:05:57.\n",
      "Batch   960 of 10,871. Elapsed: 0:06:12.\n",
      "Batch 1,000 of 10,871. Elapsed: 0:06:28.\n",
      "Batch 1,040 of 10,871. Elapsed: 0:06:43.\n",
      "Batch 1,080 of 10,871. Elapsed: 0:06:59.\n",
      "Batch 1,120 of 10,871. Elapsed: 0:07:15.\n",
      "Batch 1,160 of 10,871. Elapsed: 0:07:30.\n",
      "Batch 1,200 of 10,871. Elapsed: 0:07:46.\n",
      "Batch 1,240 of 10,871. Elapsed: 0:08:01.\n",
      "Batch 1,280 of 10,871. Elapsed: 0:08:17.\n",
      "Batch 1,320 of 10,871. Elapsed: 0:08:32.\n",
      "Batch 1,360 of 10,871. Elapsed: 0:08:48.\n",
      "Batch 1,400 of 10,871. Elapsed: 0:09:03.\n",
      "Batch 1,440 of 10,871. Elapsed: 0:09:19.\n",
      "Batch 1,480 of 10,871. Elapsed: 0:09:34.\n",
      "Batch 1,520 of 10,871. Elapsed: 0:09:50.\n",
      "Batch 1,560 of 10,871. Elapsed: 0:10:06.\n",
      "Batch 1,600 of 10,871. Elapsed: 0:10:21.\n",
      "Batch 1,640 of 10,871. Elapsed: 0:10:37.\n",
      "Batch 1,680 of 10,871. Elapsed: 0:10:52.\n",
      "Batch 1,720 of 10,871. Elapsed: 0:11:08.\n",
      "Batch 1,760 of 10,871. Elapsed: 0:11:23.\n",
      "Batch 1,800 of 10,871. Elapsed: 0:11:39.\n",
      "Batch 1,840 of 10,871. Elapsed: 0:11:54.\n",
      "Batch 1,880 of 10,871. Elapsed: 0:12:10.\n",
      "Batch 1,920 of 10,871. Elapsed: 0:12:26.\n",
      "Batch 1,960 of 10,871. Elapsed: 0:12:41.\n",
      "Batch 2,000 of 10,871. Elapsed: 0:12:57.\n",
      "Batch 2,040 of 10,871. Elapsed: 0:13:12.\n",
      "Batch 2,080 of 10,871. Elapsed: 0:13:28.\n",
      "Batch 2,120 of 10,871. Elapsed: 0:13:43.\n",
      "Batch 2,160 of 10,871. Elapsed: 0:13:59.\n",
      "Batch 2,200 of 10,871. Elapsed: 0:14:14.\n",
      "Batch 2,240 of 10,871. Elapsed: 0:14:30.\n",
      "Batch 2,280 of 10,871. Elapsed: 0:14:46.\n",
      "Batch 2,320 of 10,871. Elapsed: 0:15:01.\n",
      "Batch 2,360 of 10,871. Elapsed: 0:15:17.\n",
      "Batch 2,400 of 10,871. Elapsed: 0:15:32.\n",
      "Batch 2,440 of 10,871. Elapsed: 0:15:48.\n",
      "Batch 2,480 of 10,871. Elapsed: 0:16:03.\n",
      "Batch 2,520 of 10,871. Elapsed: 0:16:19.\n",
      "Batch 2,560 of 10,871. Elapsed: 0:16:34.\n",
      "Batch 2,600 of 10,871. Elapsed: 0:16:50.\n",
      "Batch 2,640 of 10,871. Elapsed: 0:17:06.\n",
      "Batch 2,680 of 10,871. Elapsed: 0:17:21.\n",
      "Batch 2,720 of 10,871. Elapsed: 0:17:37.\n",
      "Batch 2,760 of 10,871. Elapsed: 0:17:52.\n",
      "Batch 2,800 of 10,871. Elapsed: 0:18:08.\n",
      "Batch 2,840 of 10,871. Elapsed: 0:18:23.\n",
      "Batch 2,880 of 10,871. Elapsed: 0:18:39.\n",
      "Batch 2,920 of 10,871. Elapsed: 0:18:55.\n",
      "Batch 2,960 of 10,871. Elapsed: 0:19:10.\n",
      "Batch 3,000 of 10,871. Elapsed: 0:19:26.\n",
      "Batch 3,040 of 10,871. Elapsed: 0:19:41.\n",
      "Batch 3,080 of 10,871. Elapsed: 0:19:57.\n",
      "Batch 3,120 of 10,871. Elapsed: 0:20:12.\n",
      "Batch 3,160 of 10,871. Elapsed: 0:20:28.\n",
      "Batch 3,200 of 10,871. Elapsed: 0:20:43.\n",
      "Batch 3,240 of 10,871. Elapsed: 0:20:59.\n",
      "Batch 3,280 of 10,871. Elapsed: 0:21:15.\n",
      "Batch 3,320 of 10,871. Elapsed: 0:21:30.\n",
      "Batch 3,360 of 10,871. Elapsed: 0:21:46.\n",
      "Batch 3,400 of 10,871. Elapsed: 0:22:01.\n",
      "Batch 3,440 of 10,871. Elapsed: 0:22:17.\n",
      "Batch 3,480 of 10,871. Elapsed: 0:22:32.\n",
      "Batch 3,520 of 10,871. Elapsed: 0:22:48.\n",
      "Batch 3,560 of 10,871. Elapsed: 0:23:03.\n",
      "Batch 3,600 of 10,871. Elapsed: 0:23:19.\n",
      "Batch 3,640 of 10,871. Elapsed: 0:23:35.\n",
      "Batch 3,680 of 10,871. Elapsed: 0:23:50.\n",
      "Batch 3,720 of 10,871. Elapsed: 0:24:06.\n",
      "Batch 3,760 of 10,871. Elapsed: 0:24:21.\n",
      "Batch 3,800 of 10,871. Elapsed: 0:24:37.\n",
      "Batch 3,840 of 10,871. Elapsed: 0:24:52.\n",
      "Batch 3,880 of 10,871. Elapsed: 0:25:08.\n",
      "Batch 3,920 of 10,871. Elapsed: 0:25:23.\n",
      "Batch 3,960 of 10,871. Elapsed: 0:25:39.\n",
      "Batch 4,000 of 10,871. Elapsed: 0:25:55.\n",
      "Batch 4,040 of 10,871. Elapsed: 0:26:10.\n",
      "Batch 4,080 of 10,871. Elapsed: 0:26:26.\n",
      "Batch 4,120 of 10,871. Elapsed: 0:26:41.\n",
      "Batch 4,160 of 10,871. Elapsed: 0:26:57.\n",
      "Batch 4,200 of 10,871. Elapsed: 0:27:12.\n",
      "Batch 4,240 of 10,871. Elapsed: 0:27:28.\n",
      "Batch 4,280 of 10,871. Elapsed: 0:27:43.\n",
      "Batch 4,320 of 10,871. Elapsed: 0:27:59.\n",
      "Batch 4,360 of 10,871. Elapsed: 0:28:15.\n",
      "Batch 4,400 of 10,871. Elapsed: 0:28:30.\n",
      "Batch 4,440 of 10,871. Elapsed: 0:28:46.\n",
      "Batch 4,480 of 10,871. Elapsed: 0:29:01.\n",
      "Batch 4,520 of 10,871. Elapsed: 0:29:17.\n",
      "Batch 4,560 of 10,871. Elapsed: 0:29:32.\n",
      "Batch 4,600 of 10,871. Elapsed: 0:29:48.\n",
      "Batch 4,640 of 10,871. Elapsed: 0:30:03.\n",
      "Batch 4,680 of 10,871. Elapsed: 0:30:19.\n",
      "Batch 4,720 of 10,871. Elapsed: 0:30:35.\n",
      "Batch 4,760 of 10,871. Elapsed: 0:30:50.\n",
      "Batch 4,800 of 10,871. Elapsed: 0:31:06.\n",
      "Batch 4,840 of 10,871. Elapsed: 0:31:21.\n",
      "Batch 4,880 of 10,871. Elapsed: 0:31:37.\n",
      "Batch 4,920 of 10,871. Elapsed: 0:31:52.\n",
      "Batch 4,960 of 10,871. Elapsed: 0:32:08.\n",
      "Batch 5,000 of 10,871. Elapsed: 0:32:23.\n",
      "Batch 5,040 of 10,871. Elapsed: 0:32:39.\n",
      "Batch 5,080 of 10,871. Elapsed: 0:32:55.\n",
      "Batch 5,120 of 10,871. Elapsed: 0:33:10.\n",
      "Batch 5,160 of 10,871. Elapsed: 0:33:26.\n",
      "Batch 5,200 of 10,871. Elapsed: 0:33:41.\n",
      "Batch 5,240 of 10,871. Elapsed: 0:33:57.\n",
      "Batch 5,280 of 10,871. Elapsed: 0:34:12.\n",
      "Batch 5,320 of 10,871. Elapsed: 0:34:28.\n",
      "Batch 5,360 of 10,871. Elapsed: 0:34:43.\n",
      "Batch 5,400 of 10,871. Elapsed: 0:34:59.\n",
      "Batch 5,440 of 10,871. Elapsed: 0:35:14.\n",
      "Batch 5,480 of 10,871. Elapsed: 0:35:30.\n",
      "Batch 5,520 of 10,871. Elapsed: 0:35:46.\n",
      "Batch 5,560 of 10,871. Elapsed: 0:36:01.\n",
      "Batch 5,600 of 10,871. Elapsed: 0:36:17.\n",
      "Batch 5,640 of 10,871. Elapsed: 0:36:32.\n",
      "Batch 5,680 of 10,871. Elapsed: 0:36:48.\n",
      "Batch 5,720 of 10,871. Elapsed: 0:37:04.\n",
      "Batch 5,760 of 10,871. Elapsed: 0:37:19.\n",
      "Batch 5,800 of 10,871. Elapsed: 0:37:35.\n",
      "Batch 5,840 of 10,871. Elapsed: 0:37:50.\n",
      "Batch 5,880 of 10,871. Elapsed: 0:38:06.\n",
      "Batch 5,920 of 10,871. Elapsed: 0:38:21.\n",
      "Batch 5,960 of 10,871. Elapsed: 0:38:37.\n",
      "Batch 6,000 of 10,871. Elapsed: 0:38:52.\n",
      "Batch 6,040 of 10,871. Elapsed: 0:39:08.\n",
      "Batch 6,080 of 10,871. Elapsed: 0:39:24.\n",
      "Batch 6,120 of 10,871. Elapsed: 0:39:39.\n",
      "Batch 6,160 of 10,871. Elapsed: 0:39:55.\n",
      "Batch 6,200 of 10,871. Elapsed: 0:40:10.\n",
      "Batch 6,240 of 10,871. Elapsed: 0:40:26.\n",
      "Batch 6,280 of 10,871. Elapsed: 0:40:41.\n",
      "Batch 6,320 of 10,871. Elapsed: 0:40:57.\n",
      "Batch 6,360 of 10,871. Elapsed: 0:41:12.\n",
      "Batch 6,400 of 10,871. Elapsed: 0:41:28.\n",
      "Batch 6,440 of 10,871. Elapsed: 0:41:44.\n",
      "Batch 6,480 of 10,871. Elapsed: 0:41:59.\n",
      "Batch 6,520 of 10,871. Elapsed: 0:42:15.\n",
      "Batch 6,560 of 10,871. Elapsed: 0:42:30.\n",
      "Batch 6,600 of 10,871. Elapsed: 0:42:46.\n",
      "Batch 6,640 of 10,871. Elapsed: 0:43:01.\n",
      "Batch 6,680 of 10,871. Elapsed: 0:43:17.\n",
      "Batch 6,720 of 10,871. Elapsed: 0:43:32.\n",
      "Batch 6,760 of 10,871. Elapsed: 0:43:48.\n",
      "Batch 6,800 of 10,871. Elapsed: 0:44:04.\n",
      "Batch 6,840 of 10,871. Elapsed: 0:44:19.\n",
      "Batch 6,880 of 10,871. Elapsed: 0:44:35.\n",
      "Batch 6,920 of 10,871. Elapsed: 0:44:50.\n",
      "Batch 6,960 of 10,871. Elapsed: 0:45:06.\n",
      "Batch 7,000 of 10,871. Elapsed: 0:45:21.\n",
      "Batch 7,040 of 10,871. Elapsed: 0:45:37.\n",
      "Batch 7,080 of 10,871. Elapsed: 0:45:52.\n",
      "Batch 7,120 of 10,871. Elapsed: 0:46:08.\n",
      "Batch 7,160 of 10,871. Elapsed: 0:46:24.\n",
      "Batch 7,200 of 10,871. Elapsed: 0:46:39.\n",
      "Batch 7,240 of 10,871. Elapsed: 0:46:55.\n",
      "Batch 7,280 of 10,871. Elapsed: 0:47:10.\n",
      "Batch 7,320 of 10,871. Elapsed: 0:47:26.\n",
      "Batch 7,360 of 10,871. Elapsed: 0:47:41.\n",
      "Batch 7,400 of 10,871. Elapsed: 0:47:57.\n",
      "Batch 7,440 of 10,871. Elapsed: 0:48:12.\n",
      "Batch 7,480 of 10,871. Elapsed: 0:48:28.\n",
      "Batch 7,520 of 10,871. Elapsed: 0:48:43.\n",
      "Batch 7,560 of 10,871. Elapsed: 0:48:59.\n",
      "Batch 7,600 of 10,871. Elapsed: 0:49:15.\n",
      "Batch 7,640 of 10,871. Elapsed: 0:49:30.\n",
      "Batch 7,680 of 10,871. Elapsed: 0:49:46.\n",
      "Batch 7,720 of 10,871. Elapsed: 0:50:01.\n",
      "Batch 7,760 of 10,871. Elapsed: 0:50:17.\n",
      "Batch 7,800 of 10,871. Elapsed: 0:50:32.\n",
      "Batch 7,840 of 10,871. Elapsed: 0:50:48.\n",
      "Batch 7,880 of 10,871. Elapsed: 0:51:03.\n",
      "Batch 7,920 of 10,871. Elapsed: 0:51:19.\n",
      "Batch 7,960 of 10,871. Elapsed: 0:51:35.\n",
      "Batch 8,000 of 10,871. Elapsed: 0:51:50.\n",
      "Batch 8,040 of 10,871. Elapsed: 0:52:06.\n",
      "Batch 8,080 of 10,871. Elapsed: 0:52:21.\n",
      "Batch 8,120 of 10,871. Elapsed: 0:52:37.\n",
      "Batch 8,160 of 10,871. Elapsed: 0:52:52.\n",
      "Batch 8,200 of 10,871. Elapsed: 0:53:08.\n",
      "Batch 8,240 of 10,871. Elapsed: 0:53:23.\n",
      "Batch 8,280 of 10,871. Elapsed: 0:53:39.\n",
      "Batch 8,320 of 10,871. Elapsed: 0:53:55.\n",
      "Batch 8,360 of 10,871. Elapsed: 0:54:10.\n",
      "Batch 8,400 of 10,871. Elapsed: 0:54:26.\n",
      "Batch 8,440 of 10,871. Elapsed: 0:54:41.\n",
      "Batch 8,480 of 10,871. Elapsed: 0:54:57.\n",
      "Batch 8,520 of 10,871. Elapsed: 0:55:12.\n",
      "Batch 8,560 of 10,871. Elapsed: 0:55:28.\n",
      "Batch 8,600 of 10,871. Elapsed: 0:55:43.\n",
      "Batch 8,640 of 10,871. Elapsed: 0:55:59.\n",
      "Batch 8,680 of 10,871. Elapsed: 0:56:15.\n",
      "Batch 8,720 of 10,871. Elapsed: 0:56:30.\n",
      "Batch 8,760 of 10,871. Elapsed: 0:56:46.\n",
      "Batch 8,800 of 10,871. Elapsed: 0:57:02.\n",
      "Batch 8,840 of 10,871. Elapsed: 0:57:17.\n",
      "Batch 8,880 of 10,871. Elapsed: 0:57:33.\n",
      "Batch 8,920 of 10,871. Elapsed: 0:57:49.\n",
      "Batch 8,960 of 10,871. Elapsed: 0:58:04.\n",
      "Batch 9,000 of 10,871. Elapsed: 0:58:20.\n",
      "Batch 9,040 of 10,871. Elapsed: 0:58:35.\n",
      "Batch 9,080 of 10,871. Elapsed: 0:58:51.\n",
      "Batch 9,120 of 10,871. Elapsed: 0:59:06.\n",
      "Batch 9,160 of 10,871. Elapsed: 0:59:22.\n",
      "Batch 9,200 of 10,871. Elapsed: 0:59:37.\n",
      "Batch 9,240 of 10,871. Elapsed: 0:59:53.\n",
      "Batch 9,280 of 10,871. Elapsed: 1:00:09.\n",
      "Batch 9,320 of 10,871. Elapsed: 1:00:24.\n",
      "Batch 9,360 of 10,871. Elapsed: 1:00:40.\n",
      "Batch 9,400 of 10,871. Elapsed: 1:00:55.\n",
      "Batch 9,440 of 10,871. Elapsed: 1:01:11.\n",
      "Batch 9,480 of 10,871. Elapsed: 1:01:26.\n",
      "Batch 9,520 of 10,871. Elapsed: 1:01:42.\n",
      "Batch 9,560 of 10,871. Elapsed: 1:01:57.\n",
      "Batch 9,600 of 10,871. Elapsed: 1:02:13.\n",
      "Batch 9,640 of 10,871. Elapsed: 1:02:28.\n",
      "Batch 9,680 of 10,871. Elapsed: 1:02:44.\n",
      "Batch 9,720 of 10,871. Elapsed: 1:03:00.\n",
      "Batch 9,760 of 10,871. Elapsed: 1:03:15.\n",
      "Batch 9,800 of 10,871. Elapsed: 1:03:31.\n",
      "Batch 9,840 of 10,871. Elapsed: 1:03:46.\n",
      "Batch 9,880 of 10,871. Elapsed: 1:04:02.\n",
      "Batch 9,920 of 10,871. Elapsed: 1:04:17.\n",
      "Batch 9,960 of 10,871. Elapsed: 1:04:33.\n",
      "Batch 10,000 of 10,871. Elapsed: 1:04:48.\n",
      "Batch 10,040 of 10,871. Elapsed: 1:05:04.\n",
      "Batch 10,080 of 10,871. Elapsed: 1:05:19.\n",
      "Batch 10,120 of 10,871. Elapsed: 1:05:35.\n",
      "Batch 10,160 of 10,871. Elapsed: 1:05:51.\n",
      "Batch 10,200 of 10,871. Elapsed: 1:06:06.\n",
      "Batch 10,240 of 10,871. Elapsed: 1:06:22.\n",
      "Batch 10,280 of 10,871. Elapsed: 1:06:37.\n",
      "Batch 10,320 of 10,871. Elapsed: 1:06:53.\n",
      "Batch 10,360 of 10,871. Elapsed: 1:07:08.\n",
      "Batch 10,400 of 10,871. Elapsed: 1:07:24.\n",
      "Batch 10,440 of 10,871. Elapsed: 1:07:39.\n",
      "Batch 10,480 of 10,871. Elapsed: 1:07:55.\n",
      "Batch 10,520 of 10,871. Elapsed: 1:08:11.\n",
      "Batch 10,560 of 10,871. Elapsed: 1:08:26.\n",
      "Batch 10,600 of 10,871. Elapsed: 1:08:42.\n",
      "Batch 10,640 of 10,871. Elapsed: 1:08:57.\n",
      "Batch 10,680 of 10,871. Elapsed: 1:09:13.\n",
      "Batch 10,720 of 10,871. Elapsed: 1:09:28.\n",
      "Batch 10,760 of 10,871. Elapsed: 1:09:44.\n",
      "Batch 10,800 of 10,871. Elapsed: 1:09:59.\n",
      "Batch 10,840 of 10,871. Elapsed: 1:10:15.\n",
      "\n",
      "Average training loss: 0.00\n",
      "Training epoch took: 1:10:27\n",
      "\n",
      "Running Validation...\n",
      "Validation loss: 0.0014753183331104524\n",
      "Validation Accuracy: 0.9996271135976547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [2:32:50<00:00, 4585.22s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.9901484206051421\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch_i in trange(epochs, desc='Epoch'):\n",
    "    ########################################\n",
    "    ## Training\n",
    "    ########################################\n",
    "    print()\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         b_input_ids = torch.tensor(b_input_ids).to(torch.int64)\n",
    "#         b_input_mask = torch.tensor(b_input_mask).to(torch.int64)\n",
    "#         b_labels = torch.tensor(b_labels).to(torch.int64)\n",
    "        \n",
    "        b_input_ids = batch['word_tokens'].to(device).to(torch.int64)\n",
    "        b_input_mask = batch['masks'].to(device).to(torch.int64)\n",
    "        b_labels = batch['tags'].to(device).to(torch.int64)\n",
    "    \n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print()\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "#     avg_train_loss = total_loss / len(train_dataloader)\n",
    "#     print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    ########################################\n",
    "    ## Validate\n",
    "    ########################################\n",
    "    print()\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         b_input_ids = torch.tensor(b_input_ids).to(torch.int64)\n",
    "#         b_input_mask = torch.tensor(b_input_mask).to(torch.int64)\n",
    "#         b_labels = torch.tensor(b_labels).to(torch.int64)\n",
    "\n",
    "        b_input_ids = batch['word_tokens'].to(device).to(torch.int64)\n",
    "        b_input_mask = batch['masks'].to(device).to(torch.int64)\n",
    "        b_labels = batch['tags'].to(device).to(torch.int64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "    \n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    \n",
    "    pred_tags = []\n",
    "    for p, l in zip(predictions, true_labels):\n",
    "        row = [tag_values[p_i] for p_i, l_i in zip(p, l) if tag_values[l_i] != 'PAD']\n",
    "        pred_tags.append(row)\n",
    "    \n",
    "    valid_tags = []\n",
    "    for l in true_labels:\n",
    "        row = [tag_values[l_i] for l_i in l if tag_values[l_i] != 'PAD']\n",
    "        valid_tags.append(row)\n",
    "    \n",
    "    print('Validation Accuracy: {}'.format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print('Validation F1-Score: {}'.format(f1_score(pred_tags, valid_tags)))\n",
    "    print()\n",
    "\n",
    "print('')\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./pytorch_bert_ner_model_v2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAIN_MODEL)\n",
    "model = BertForTokenClassification.from_pretrained('C:\\\\Users\\\\stick\\\\kaggle\\\\pytorch_bert_ner_model_v2')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2396710, 3),\n",
       "          Sentence          Word Tag\n",
       " 0        S2682021       SeaWiFS   O\n",
       " 1        S2682021   ORM-derived   O\n",
       " 2        S2682021             g   O\n",
       " 3        S2682021             i   O\n",
       " 4        S2682021         ͑443͒   O\n",
       " ...           ...           ...  ..\n",
       " 2396705   S794046             a   O\n",
       " 2396706   S794046  multiplicity   O\n",
       " 2396707   S794046            of   O\n",
       " 2396708   S794046      purposes   O\n",
       " 2396709   S794046             .   O\n",
       " \n",
       " [2396710 rows x 3 columns])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = pd.read_csv(\"C:\\\\Users\\\\stick\\\\kaggle\\\\ner_test.csv\",sep=\",\",encoding=\"utf8\", header=None, keep_default_na=False, na_values=[], names=['Sentence', 'Word', 'Tag'])\n",
    "testing_data.shape, testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_getter = SentenceGetter(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Pipeline', '2', ',', 'the', 'group-level', 'correlation', 'maps', 'of', 'each', 'ROI', 'were', 'saved', 'to', 'a', 'binary', 'mask', '(', 'with', 'positive', 'and', 'negative', 'functional', 'connectivity', 'separated', ')', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "sentences = [[tu[0] for tu in sentence] for sentence in testing_getter.sentences]\n",
    "print(sentences[0])\n",
    "labels = [[tu[1] for tu in sentence] for sentence in testing_getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(sentences, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['In',\n",
       "   'Pi',\n",
       "   '##pel',\n",
       "   '##ine',\n",
       "   '2',\n",
       "   ',',\n",
       "   'the',\n",
       "   'group',\n",
       "   '-',\n",
       "   'level',\n",
       "   'correlation',\n",
       "   'maps',\n",
       "   'of',\n",
       "   'each',\n",
       "   'R',\n",
       "   '##O',\n",
       "   '##I',\n",
       "   'were',\n",
       "   'saved',\n",
       "   'to',\n",
       "   'a',\n",
       "   'binary',\n",
       "   'mask',\n",
       "   '(',\n",
       "   'with',\n",
       "   'positive',\n",
       "   'and',\n",
       "   'negative',\n",
       "   'functional',\n",
       "   'connectivity',\n",
       "   'separated',\n",
       "   ')',\n",
       "   '.'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O']),\n",
       " (['The',\n",
       "   'structural',\n",
       "   'and',\n",
       "   'resting',\n",
       "   'state',\n",
       "   'functional',\n",
       "   'MR',\n",
       "   '##I',\n",
       "   'data',\n",
       "   'of',\n",
       "   'MC',\n",
       "   '##I',\n",
       "   'patients',\n",
       "   'used',\n",
       "   'in',\n",
       "   'the',\n",
       "   'present',\n",
       "   'study',\n",
       "   'were',\n",
       "   'obtained',\n",
       "   'from',\n",
       "   'large',\n",
       "   'multi',\n",
       "   '##cent',\n",
       "   '##er',\n",
       "   'Alzheimer',\n",
       "   \"'\",\n",
       "   's',\n",
       "   'Disease',\n",
       "   'N',\n",
       "   '##eur',\n",
       "   '##oi',\n",
       "   '##maging',\n",
       "   'Initiative',\n",
       "   '(',\n",
       "   'AD',\n",
       "   '##NI',\n",
       "   ')',\n",
       "   'studies',\n",
       "   '3',\n",
       "   '.'],\n",
       "  ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'I-D',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts_and_labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(\n",
    "    [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], \n",
    "    maxlen=MAX_LEN, \n",
    "    dtype='long', \n",
    "    value=0.0, \n",
    "    truncating=TRUNCATING_TYPE, \n",
    "    padding=PADDING_TYPE\n",
    ")\n",
    "tags = pad_sequences(\n",
    "    [[tag2idx.get(l) for l in lab] for lab in labels], \n",
    "    maxlen=MAX_LEN, \n",
    "    dtype='long',\n",
    "    value=tag2idx[\"PAD\"],\n",
    "    truncating=TRUNCATING_TYPE,\n",
    "    padding=PADDING_TYPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = SeanDataset(\n",
    "    word_tokens=input_ids,\n",
    "    masks=attention_masks,\n",
    "    tags=tags\n",
    ")\n",
    "    \n",
    "testing_dataloader = DataLoader(\n",
    "    testing_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_tokens': tensor([[ 1130, 21902, 10522,  ...,     0,     0,     0],\n",
      "        [ 1109,  8649,  1105,  ...,     0,     0,     0],\n",
      "        [ 7154,  1215,  1107,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 7549,  3622,  1104,  ...,     0,     0,     0],\n",
      "        [ 1438,   117,  1142,  ...,     0,     0,     0],\n",
      "        [ 7154,  1215,  1107,  ...,     0,     0,     0]]), 'masks': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]]), 'tags': tensor([[0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3],\n",
      "        [0, 0, 0,  ..., 3, 3, 3]])}\n"
     ]
    }
   ],
   "source": [
    "for d in testing_dataloader:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1510"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.0015102616006626422\n",
      "Testing Accuracy: 0.9996206797862479\n",
      "Testing F1-Score: 0.9905139646339448\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "testing_loss, testing_accuracy = 0, 0\n",
    "nb_testing_steps, nb_testing_examples = 0, 0\n",
    "predictions, true_labels = [], []\n",
    "testing_loss_values = []\n",
    "for batch in testing_dataloader:\n",
    "    b_input_ids = batch['word_tokens'].to(device).to(torch.int64)\n",
    "    b_input_mask = batch['masks'].to(device).to(torch.int64)\n",
    "    b_labels = batch['tags'].to(device).to(torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    testing_loss += outputs[0].mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "testing_loss = testing_loss / len(testing_dataloader)\n",
    "validation_loss_values.append(testing_loss)\n",
    "print(\"Testing loss: {}\".format(testing_loss))\n",
    "\n",
    "pred_tags = []\n",
    "for p, l in zip(predictions, true_labels):\n",
    "    row = [tag_values[p_i] for p_i, l_i in zip(p, l) if tag_values[l_i] != 'PAD']\n",
    "    pred_tags.append(row)\n",
    "\n",
    "testing_tags = []\n",
    "for l in true_labels:\n",
    "    row = [tag_values[l_i] for l_i in l if tag_values[l_i] != 'PAD']\n",
    "    testing_tags.append(row)\n",
    "\n",
    "print('Testing Accuracy: {}'.format(accuracy_score(pred_tags, testing_tags)))\n",
    "print('Testing F1-Score: {}'.format(f1_score(pred_tags, testing_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate testing dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual input inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAIN_MODEL)\n",
    "model = BertForTokenClassification.from_pretrained('C:\\\\Users\\\\stick\\\\kaggle\\\\pytorch_bert_ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_tag(tokenized_sentence, pred_tags):\n",
    "    dataset_names = []\n",
    "    dataset_name = ''\n",
    "    for (token, tag) in zip(tokenized_sentence, pred_tags):\n",
    "        if tag == 'B-D' or tag == 'I-D':\n",
    "            if token.startswith('##'):\n",
    "                dataset_name += token.replace('##', '')\n",
    "            else:\n",
    "                dataset_name += ' ' + token\n",
    "        else:\n",
    "            if len(dataset_name) > 0:\n",
    "                dataset_names.append(dataset_name.strip())\n",
    "                dataset_name = ''\n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(f'C:\\\\Users\\\\stick\\\\kaggle\\\\0087b0b4-deda-471a-a8b7-706b9dc24990.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "    for section in data:\n",
    "        # Skip section_title\n",
    "#             section_title = section['section_title']\n",
    "#             if '\\n' not in section_title:\n",
    "#                 paras.append(section_title)\n",
    "#             else:\n",
    "#                 paras.extend(section_title.split('\\n'))\n",
    "\n",
    "        text = section['text']\n",
    "        section_sentences = nltk.sent_tokenize(text)\n",
    "        for s in section_sentences:\n",
    "            sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Survey of Earned Doctorates']\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tokenized_sentence = []\n",
    "    for w in words:\n",
    "        tokenized_word = tokenizer.tokenize(w)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        \n",
    "    # print(tokenized_sentence)\n",
    "    input_ids = pad_sequences(\n",
    "        [tokenizer.convert_tokens_to_ids(tokenized_sentence)], \n",
    "        maxlen=MAX_LEN, \n",
    "        dtype='long', \n",
    "        value=0.0, \n",
    "        truncating=TRUNCATING_TYPE, \n",
    "        padding=PADDING_TYPE\n",
    "    )\n",
    "    # print(input_ids)\n",
    "    \n",
    "    attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "    \n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long),\n",
    "    attention_masks = torch.tensor(attention_masks, dtype=torch.float),\n",
    "    # print(attention_masks)\n",
    "    \n",
    "    model.eval()\n",
    "    b_input_ids = input_ids[0].to(device).to(torch.int64)\n",
    "    b_input_mask = attention_masks[0].to(device).to(torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs[0].detach().cpu().numpy()\n",
    "#     print(logits)\n",
    "    predictions = np.argmax(logits, axis=2)[0]\n",
    "#     print(predictions)\n",
    "    pred_tags = [tag_values[w] for w in predictions]\n",
    "    if 'B-D' in pred_tags or 'I-D' in pred_tags:\n",
    "        dataset_names = get_dataset_tag(tokenized_sentence, pred_tags)\n",
    "        print(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341b1f10dffd457ead74b8e3ac95a5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0fd6f4a5aa44598efb847477239730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=435797.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAIN_MODEL)\n",
    "config = AutoConfig.from_pretrained(PRETRAIN_MODEL)\n",
    "\n",
    "tokenizer.save_pretrained('C:\\\\Users\\\\stick\\\\kaggle\\\\tokenizer')\n",
    "config.save_pretrained('C:\\\\Users\\\\stick\\\\kaggle\\\\tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xji",
   "language": "python",
   "name": "xji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
