{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training files: 14316\n",
      "All checked\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Verify to make sure we understand the json format properly\n",
    "# Each json file contains one root list, each item in the list has two parts:\n",
    "# A section_title and a text\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import json\n",
    "total_training_files = 0\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/coleridgeinitiative-show-us-the-data/train'):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        with open(file_path) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            total_training_files += 1\n",
    "            for section in data:\n",
    "                if len(section) != 2:\n",
    "                    print(file_path)\n",
    "#         section_title = section['section_title']\n",
    "#         text = section['text']\n",
    "#         print('----section----')\n",
    "#         print(section)\n",
    "\n",
    "print(f'Total training files: {total_training_files}')\n",
    "print('All checked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '/kaggle/input/coleridgeinitiative-show-us-the-data/train.csv',\n",
    "    index_col=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worthy_negative(sentence):\n",
    "    # If a sentence contains Xxxx Xxxx Xxxx or (XYZ) then output it as negative sample\n",
    "    bracket_pattern = re.compile(re.escape('(')+'[A-Z][A-Z][A-Z]+'+re.escape(')'))\n",
    "    capital_pattern = re.compile('[A-Z][A-Za-z0-9]'+re.escape(' ')+'[A-Z][A-Za-z0-9]+')\n",
    "    \n",
    "    if bracket_pattern.search(sentence) is not None:\n",
    "        return True\n",
    "    \n",
    "    if capital_pattern.search(sentence) is not None:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# For each fid, get all dataset_labels\n",
    "def get_fid_labels_dict():\n",
    "    processed = 0\n",
    "    fid_labels_dict = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        processed += 1\n",
    "        if processed % 2000 == 0:\n",
    "            print(f'Processed {processed} documents')\n",
    "            \n",
    "        fid = row['Id']\n",
    "        dataset_label = row['dataset_label']\n",
    "        \n",
    "        if fid not in fid_labels_dict:\n",
    "            fid_labels_dict[fid] = set()\n",
    "            \n",
    "        fid_labels_dict[fid].add(dataset_label)\n",
    "        \n",
    "    return fid_labels_dict\n",
    "\n",
    "def generate_sentence_label_dataset(save_csv_file_path):\n",
    "    fid_labels_dict = get_fid_labels_dict()\n",
    "    \n",
    "#     sentences = []\n",
    "    sid = 0\n",
    "    processed = 0\n",
    "    with open(save_csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            processed += 1\n",
    "            if processed % 2000 == 0:\n",
    "                print(f'Processed {processed} documents')    \n",
    "\n",
    "            fid = row['Id']\n",
    "            labels = fid_labels_dict[fid]        \n",
    "\n",
    "            with open(f'/kaggle/input/coleridgeinitiative-show-us-the-data/train/{fid}.json') as json_file:\n",
    "                data = json.load(json_file)\n",
    "\n",
    "                for section in data:\n",
    "                    text = section['text']\n",
    "                    section_sentences = nltk.sent_tokenize(text)\n",
    "                    for s in section_sentences:\n",
    "                        # For each sentence, find contained labels, if multiple labels are found, put all into the contained_labels list\n",
    "                        contained_labels = []\n",
    "                        for label in labels:\n",
    "                            if label in s:\n",
    "                                contained_labels.append(label)\n",
    "\n",
    "                        if len(contained_labels) > 0:\n",
    "                            writer.writerow([fid, f'S{sid}', s, '||'.join(contained_labels)])\n",
    "                        elif worthy_negative(s):\n",
    "                            writer.writerow([fid, f'S{sid}', s, None])\n",
    "\n",
    "                        sid += 1\n",
    "\n",
    "    print(f'Total training sentences: {sid+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence_label_dataset('./ner_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_dedup(sentence, label_list):\n",
    "    occurances = []\n",
    "    for label in label_list:\n",
    "        for match in re.finditer(re.escape(label), sentence):\n",
    "            span = match.span()\n",
    "            if span is not None:\n",
    "                occurances.append((label, span[0], span[1]))\n",
    "                \n",
    "    occurances.sort(key=lambda x:(x[1],0-len(x[0])))\n",
    "#     print(occurances)\n",
    "    dedup_list = set()\n",
    "    ancher = None\n",
    "    for index, o in enumerate(occurances):\n",
    "        if ancher is None:\n",
    "            ancher = o\n",
    "            dedup_list.add(ancher[0])\n",
    "        else:\n",
    "            if o[1] >= ancher[2]: # Not overlapping with ancher, update ancher\n",
    "                to_be_removed = set()\n",
    "                need_to_update = True\n",
    "                for d in dedup_list:\n",
    "                    if o[0] in d:\n",
    "                        need_to_update = False\n",
    "                        break\n",
    "                    elif d in o[0]:\n",
    "                        to_be_removed.add(d)\n",
    "                        \n",
    "                if len(to_be_removed) > 0:\n",
    "                    for rm in to_be_removed:\n",
    "                        dedup_list.remove(rm)\n",
    "                        \n",
    "                if need_to_update:\n",
    "                    ancher = o\n",
    "                    dedup_list.add(o[0])\n",
    "                \n",
    "    if ancher is not None:\n",
    "        dedup_list.add(ancher[0])\n",
    "                \n",
    "    return dedup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentence_df = pd.read_csv(\n",
    "    './ner_sentences.csv',\n",
    "    index_col=None,\n",
    "    header=None,\n",
    "    keep_default_na=False,\n",
    "    names=['fid', 'sid', 'sentence', 'labels'],\n",
    "    dtype={\n",
    "        'fid': 'str',\n",
    "        'sid': 'str', \n",
    "        'sentence': 'str',\n",
    "        'labels': 'str',\n",
    "    },\n",
    ")\n",
    "\n",
    "# Dedup overlapping labels for each sentence\n",
    "dedup_labels = []\n",
    "processed = 0\n",
    "for index, row in sentence_df.iterrows():\n",
    "    processed += 1\n",
    "    if processed % 50000 == 0:\n",
    "        print(f'Processed {processed} documents')\n",
    "\n",
    "    sid = row['sid']\n",
    "    labels = row['labels']\n",
    "    sentence = row['sentence']\n",
    "    if labels is None or labels == '':\n",
    "        dedup_labels.append(None)\n",
    "    else:\n",
    "        dedup_label_list = label_dedup(sentence, labels.split('||'))\n",
    "        dedup_labels.append('||'.join(dedup_label_list))\n",
    "        \n",
    "sentence_df['dedup_labels'] = dedup_labels\n",
    "\n",
    "# Split training and testing\n",
    "x_train,x_test = train_test_split(sentence_df, test_size=0.10, random_state=2021)\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "\n",
    "x_train.to_csv('./sentence_training.csv', na_rep='', index=False)\n",
    "x_test.to_csv('./sentence_test.csv', na_rep='', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    './sentence_training.csv',\n",
    "    index_col=None,\n",
    "    header='infer',\n",
    "    keep_default_na=False,\n",
    "    dtype={\n",
    "        'fid': 'str',\n",
    "        'sid': 'str', \n",
    "        'sentence': 'str',\n",
    "        'labels': 'str',\n",
    "        'dedup_labels': 'str'\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurances(word_tokens, s):\n",
    "#     print(list_of_words)\n",
    "#     print(s)\n",
    "    ret = []\n",
    "    start_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            i = word_tokens.index(s, start_pos)\n",
    "            ret.append(i)\n",
    "            start_pos = i+1\n",
    "        except ValueError:\n",
    "            return ret\n",
    "\n",
    "# Given a sentence as a list of words and a list of words for dataset_label, update the given tag sequence \n",
    "def update_tags(word_tokens, label_tokens, tags):\n",
    "    try:\n",
    "        first_word_indice = get_occurances(word_tokens, label_tokens[0])\n",
    "    #     print(first_word_indice)\n",
    "        for m in first_word_indice:\n",
    "            match = True\n",
    "            p = m\n",
    "            for i in range(len(label_tokens)):\n",
    "                if p < len(word_tokens) and word_tokens[p] == label_tokens[i]:\n",
    "                    p+=1\n",
    "                else:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "    #             print(f'match {m}')\n",
    "                assert tags[m] == 'O', 'Label overlap!'\n",
    "                tags[m] = 'B-D'\n",
    "                m+=1\n",
    "                for i in range(1, len(label_tokens)):\n",
    "                    assert tags[m] == 'O', 'Label overlap!'\n",
    "                    tags[m] = 'I-D'\n",
    "                    m+=1\n",
    "        return tags\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "        print(words)\n",
    "        print(words_label)\n",
    "\n",
    "def generate_word_tag_dataset(input_file_path, output_file_path):\n",
    "    df = pd.read_csv(\n",
    "        input_file_path,\n",
    "        index_col=None,\n",
    "        keep_default_na=False,\n",
    "        header='infer',\n",
    "        dtype={\n",
    "            'fid': 'str',\n",
    "            'sid': 'str', \n",
    "            'sentence': 'str',\n",
    "            'labels': 'str',\n",
    "            'dedup_labels': 'str'\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "    \n",
    "        for index, row in df.iterrows():\n",
    "            sid = row['sid']\n",
    "            sentence = row['sentence']\n",
    "            labels = row['dedup_labels']\n",
    "\n",
    "            # Create tags for each word in a sentence\n",
    "            word_tokens = word_tokenize(sentence)\n",
    "            tags = ['O']*len(word_tokens)\n",
    "            \n",
    "            if labels is not None and labels > '': # If there are labels\n",
    "                for l in labels.split('||'):\n",
    "                    # Update the tag sequence with the current label\n",
    "                    label_tokens = word_tokenize(l)\n",
    "                    try:\n",
    "                        tags = update_tags(word_tokens, label_tokens, tags)\n",
    "                    except AssertionError as ae:\n",
    "                        print(f'{sid} label overlap!')\n",
    "                        \n",
    "            for (w,t) in zip(word_tokens, tags):\n",
    "                writer.writerow([f'{sid}',w,t])\n",
    "\n",
    "generate_word_tag_dataset('./sentence_training.csv', './ner_training.csv')\n",
    "generate_word_tag_dataset('./sentence_test.csv', './ner_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_df = pd.read_csv(\n",
    "    './ner_training.csv',\n",
    "    index_col=None,\n",
    "    header=None,\n",
    "    names=['sid', 'word', 'tag'],\n",
    "    keep_default_na=False,\n",
    "    dtype={\n",
    "        'sid': 'str', \n",
    "        'word': 'str',\n",
    "        'tag': 'str',\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
